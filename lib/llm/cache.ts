/**
 * LLM Cache
 *
 * Provides deterministic caching for LLM outputs to ensure reproducibility.
 * Cache is stored in /data/llm-cache/{YYYY-MM}.json files.
 */

import * as fs from "fs/promises"
import * as path from "path"
import * as crypto from "crypto"

// =============================================================================
// Types
// =============================================================================

export interface LLMCacheEntry {
  text: string
  model: string
  generatedAt: string
}

export type LLMCacheFile = Record<string, LLMCacheEntry>

// =============================================================================
// Configuration
// =============================================================================

const CACHE_DIR = path.join(process.cwd(), "data", "llm-cache")

// =============================================================================
// Cache Key Generation
// =============================================================================

/**
 * Generate a deterministic hash of data for cache key
 */
export function hashData(data: Record<string, unknown>): string {
  // Sort keys for deterministic serialization
  const sortedKeys = Object.keys(data).sort()
  const sortedObj: Record<string, unknown> = {}
  for (const key of sortedKeys) {
    sortedObj[key] = data[key]
  }
  const sorted = JSON.stringify(sortedObj)
  return crypto.createHash("sha256").update(sorted).digest("hex").slice(0, 16)
}

/**
 * Generate a deterministic cache key for an LLM output
 */
export function getLLMCacheKey(
  date: string,
  episodeType: string,
  topicSlug: string,
  questionSlot: string,
  templateId: string,
  contentType: "explanation" | "rephrase",
  dataHash: string
): string {
  return `${date}|${episodeType}|${topicSlug}|${questionSlot}|${templateId}|${contentType}|${dataHash}`
}

// =============================================================================
// File Operations
// =============================================================================

/**
 * Get the cache file path for a given date
 */
function getCacheFilePath(date: string): string {
  const [year, month] = date.split("-")
  return path.join(CACHE_DIR, `${year}-${month}.json`)
}

/**
 * Ensure the cache directory exists
 */
async function ensureCacheDir(): Promise<void> {
  try {
    await fs.mkdir(CACHE_DIR, { recursive: true })
  } catch {
    // Directory may already exist
  }
}

/**
 * Load cache file for a given date
 */
async function loadCacheFile(date: string): Promise<LLMCacheFile> {
  const filePath = getCacheFilePath(date)
  try {
    const content = await fs.readFile(filePath, "utf-8")
    return JSON.parse(content) as LLMCacheFile
  } catch {
    // File doesn't exist or is invalid - return empty cache
    return {}
  }
}

/**
 * Save cache file for a given date
 */
async function saveCacheFile(date: string, cache: LLMCacheFile): Promise<void> {
  await ensureCacheDir()
  const filePath = getCacheFilePath(date)
  await fs.writeFile(filePath, JSON.stringify(cache, null, 2), "utf-8")
}

// =============================================================================
// Cache Operations
// =============================================================================

// In-memory cache for the current session
let sessionCache: LLMCacheFile = {}
let sessionCacheDate: string | null = null

/**
 * Get a cached LLM output
 *
 * @param date - Episode date (YYYY-MM-DD)
 * @param key - Cache key generated by getLLMCacheKey
 * @returns Cached entry or null if not found
 */
export async function getCached(
  date: string,
  key: string
): Promise<LLMCacheEntry | null> {
  // Check session cache first
  if (sessionCacheDate === date && sessionCache[key]) {
    return sessionCache[key]
  }

  // Load from file
  const fileCache = await loadCacheFile(date)
  
  // Update session cache
  sessionCache = fileCache
  sessionCacheDate = date

  return fileCache[key] || null
}

/**
 * Store an LLM output in cache
 *
 * @param date - Episode date (YYYY-MM-DD)
 * @param key - Cache key generated by getLLMCacheKey
 * @param text - Generated text to cache
 * @param model - Model that generated the text
 */
export async function setCache(
  date: string,
  key: string,
  text: string,
  model: string
): Promise<void> {
  // Load current cache
  let fileCache = await loadCacheFile(date)

  // Add new entry
  const entry: LLMCacheEntry = {
    text,
    model,
    generatedAt: new Date().toISOString(),
  }

  fileCache[key] = entry

  // Update session cache
  sessionCache = fileCache
  sessionCacheDate = date

  // Save to file
  await saveCacheFile(date, fileCache)
}

/**
 * Clear the session cache (for testing)
 */
export function clearSessionCache(): void {
  sessionCache = {}
  sessionCacheDate = null
}

/**
 * Get cache statistics for a date
 */
export async function getCacheStats(date: string): Promise<{
  entryCount: number
  fileExists: boolean
  filePath: string
}> {
  const filePath = getCacheFilePath(date)
  const cache = await loadCacheFile(date)
  
  let fileExists = false
  try {
    await fs.access(filePath)
    fileExists = true
  } catch {
    fileExists = false
  }

  return {
    entryCount: Object.keys(cache).length,
    fileExists,
    filePath,
  }
}
